---
citekey: zhaoStreamingVideoModel2023
aliases:
  - Zhao et al. (2023) Streaming Video Model
title: Streaming Video Model
authors:
  - Yucheng Zhao
  - Chong Luo
  - Chuanxin Tang
  - Dongdong Chen
  - Noel Codella
  - Zheng-Jun Zha
date: 30 March 2023
item-type: Preprint
publisher: ""
comments:
  - "Comment: Accepted by CVPR'23"
tags:
  - ComputerVision
  - ComputerScience
  - video
doi: https://doi.org/10.48550/arXiv.2303.17228
draft: false
---

> [!Cite]
> Zhao Y, Luo C, Tang C, et al. (2023) Streaming Video Model. arXiv:2303.17228. arXiv. Available at: [http://arxiv.org/abs/2303.17228](http://arxiv.org/abs/2303.17228) (accessed 9 May 2024).

> [!LINK] 
> http://arxiv.org/abs/2303.17228

> [!Abstract]
>
> Video understanding tasks have traditionally been modeled by two separate architectures, specially tailored for two distinct tasks. Sequence-based video tasks, such as action recognition, use a video backbone to directly extract spatiotemporal features, while frame-based video tasks, such as multiple object tracking (MOT), rely on single fixed-image backbone to extract spatial features. In contrast, we propose to unify video understanding tasks into one novel streaming video architecture, referred to as Streaming Vision Transformer (S-ViT). S-ViT first produces frame-level features with a memory-enabled temporally-aware spatial encoder to serve the frame-based video tasks. Then the frame features are input into a task-related temporal decoder to obtain spatiotemporal features for sequence-based tasks. The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task. We believe that the concept of streaming video model and the implementation of S-ViT are solid steps towards a unified deep learning architecture for video understanding. Code will be available at https://github.com/yuzhms/Streaming-Video-Model.
>.
> 
# Notes

## Annotations
