<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <meta property="og:url" content="https://movingpixel.net/twitter/dchavezheras/status/1329360617862352896" />
  <meta property="og:title" content="Daniel Ch치vez Heras on Twitter (archived)" />
  <meta property="og:description" content="And there is also some unusual camera data that might be of interest to two different research communities: #creativeAI  @MrsBunz @eva33313 @gabrielopereira @brunomoreschi &amp; #ComputerVision @drfeifei  @jcjohnss @RanjayKrishna" />
  
  

  <title>And there is also some unusual camera data that might be of interest to two different research communities: #creativeAI  @MrsBunz @eva33313 @gabrielopereira @brunomoreschi &amp; #ComputerVision @drfeifei  @jcjohnss @RanjayKrishna</title>
  <link rel="stylesheet" href="../../../styles.css">
</head>
<body>
  <div class="wrapper">
  	<div class="flex-wrap">
      <a href="../../../">
        <p>&larr; @dchavezheras Twitter archive</p>
      </a>
      
  	  <article class="tweet parent " >
  	    <p class="display_name">
  	      Daniel Ch치vez Heras
  	    </p>
  	    <p class="user_name">
  	      @dchavezheras
  	    </p>
  	    <p class="full_text">
  	      Does machine vision apprehend the world through a specific lens? And if so, which one?<br><br>Very happy to share my latest article with @tobias_blanke in @SpringerNature AI &amp; Society 游녤<a href="https://doi.org/10.1007/s00146-020-01091-y">https://doi.org/10.1007/s00146-020-01091-y</a> #AI #photography #aesthetics
  	    </p>
  	    <p class="created_at">
  	      19/11/2020, 07:51:50
  	    </p>
  	    <p class="favorite_count">Favs: 19</p>
  	    <p class="retweet_count">Retweets: 5</p>
  	    <a class="permalink" href="../1329331537314123778">link</a>
  	  </article>


  	  <article class="tweet parent " >
  	    <p class="display_name">
  	      Daniel Ch치vez Heras
  	    </p>
  	    <p class="user_name">
  	      @dchavezheras
  	    </p>
  	    <p class="full_text">
  	      By "specific lens" we mean a historically situated point of view, but we also mean a concrete device: a photographic lens, or more precisely what in French is called "objectif" <div class="gallery"><ul><li><a href="../../tweets_media/1329341967357018112-EnLFClxUYAApjBZ.png"><img src="../../tweets_media/1329341967357018112-EnLFClxUYAApjBZ.png"></a></li></ul></div>
  	    </p>
  	    <p class="created_at">
  	      19/11/2020, 08:33:17
  	    </p>
  	    <p class="favorite_count">Favs: 3</p>
  	    <p class="retweet_count">Retweets: 0</p>
  	    <a class="permalink" href="../1329341967357018112">link</a>
  	  </article>


  	  <article class="tweet parent " >
  	    <p class="display_name">
  	      Daniel Ch치vez Heras
  	    </p>
  	    <p class="user_name">
  	      @dchavezheras
  	    </p>
  	    <p class="full_text">
  	      In critical machine vision, significant attention has been paid to datasets; their constitution, misappropriation and inherent biases, e.g. the work of @katecrawford @adamhrv And there is also recent great work on subjectivity and visibility by @TomaszHollanek @_mouchette
  	    </p>
  	    <p class="created_at">
  	      19/11/2020, 08:49:41
  	    </p>
  	    <p class="favorite_count">Favs: 3</p>
  	    <p class="retweet_count">Retweets: 0</p>
  	    <a class="permalink" href="../1329346095957229568">link</a>
  	  </article>


  	  <article class="tweet parent " >
  	    <p class="display_name">
  	      Daniel Ch치vez Heras
  	    </p>
  	    <p class="user_name">
  	      @dchavezheras
  	    </p>
  	    <p class="full_text">
  	      Our work expands the critique of machine vision beyond datasets and pushes it in a different direction, by tracing a technical genealogy that links inductive computing with analogue computing; machine learning with optics
  	    </p>
  	    <p class="created_at">
  	      19/11/2020, 09:14:42
  	    </p>
  	    <p class="favorite_count">Favs: 2</p>
  	    <p class="retweet_count">Retweets: 0</p>
  	    <a class="permalink" href="../1329352390525403136">link</a>
  	  </article>


  	  <article class="tweet parent " >
  	    <p class="display_name">
  	      Daniel Ch치vez Heras
  	    </p>
  	    <p class="user_name">
  	      @dchavezheras
  	    </p>
  	    <p class="full_text">
  	      With this in mind, we set out to design a system that is purposefully blind to what photographs are of; a type of machine vision that cares nothing about recognising objects, people or scenes, and is instead programmed to learn only about how photographs were made <div class="gallery"><ul><li><a href="../../tweets_media/1329356112538206209-EnLSJPkUwAAm0gn.png"><img src="../../tweets_media/1329356112538206209-EnLSJPkUwAAm0gn.png"></a></li><li><a href="../../tweets_media/1329356112538206209-EnLSK6LVEAEgFj8.png"><img src="../../tweets_media/1329356112538206209-EnLSK6LVEAEgFj8.png"></a></li></ul></div>
  	    </p>
  	    <p class="created_at">
  	      19/11/2020, 09:29:30
  	    </p>
  	    <p class="favorite_count">Favs: 2</p>
  	    <p class="retweet_count">Retweets: 0</p>
  	    <a class="permalink" href="../1329356112538206209">link</a>
  	  </article>


  	  <article class="tweet  " id="main">
  	    <p class="display_name">
  	      Daniel Ch치vez Heras
  	    </p>
  	    <p class="user_name">
  	      @dchavezheras
  	    </p>
  	    <p class="full_text">
  	      And there is also some unusual camera data that might be of interest to two different research communities: #creativeAI  @MrsBunz @eva33313 @gabrielopereira @brunomoreschi &amp; #ComputerVision @drfeifei  @jcjohnss @RanjayKrishna
  	    </p>
  	    <p class="created_at">
  	      19/11/2020, 09:47:24
  	    </p>
  	    <p class="favorite_count">Favs: 2</p>
  	    <p class="retweet_count">Retweets: 0</p>
  	    <a class="permalink" href="../1329360617862352896">link</a>
  	  </article>


  	  <article class="tweet  child" >
  	    <p class="display_name">
  	      Daniel Ch치vez Heras
  	    </p>
  	    <p class="user_name">
  	      @dchavezheras
  	    </p>
  	    <p class="full_text">
  	      Finally, a shout out to the wonderful people from @bbcirfs who invited me to participate in Made by Machine: @georgie @mistertim @fiala__ <div class="gallery"><ul><li><a href="../../tweets_media/1329361695592988675-EnLXPhBVEAI3o8i.png"><img src="../../tweets_media/1329361695592988675-EnLXPhBVEAI3o8i.png"></a></li></ul></div>
  	    </p>
  	    <p class="created_at">
  	      19/11/2020, 09:51:41
  	    </p>
  	    <p class="favorite_count">Favs: 2</p>
  	    <p class="retweet_count">Retweets: 0</p>
  	    <a class="permalink" href="../1329361695592988675">link</a>
  	  </article>


  	  <article class="tweet  child" >
  	    <p class="display_name">
  	      Daniel Ch치vez Heras
  	    </p>
  	    <p class="user_name">
  	      @dchavezheras
  	    </p>
  	    <p class="full_text">
  	      And to Geoff Cox, @LeoImpett and Mitra Azar who organised the Ways of Machine Seeing especial issue of AI &amp; Soc.
  	    </p>
  	    <p class="created_at">
  	      19/11/2020, 09:55:50
  	    </p>
  	    <p class="favorite_count">Favs: 0</p>
  	    <p class="retweet_count">Retweets: 0</p>
  	    <a class="permalink" href="../1329362741849591809">link</a>
  	  </article>

  	</div>
  </div>
</body>
<script>
document.getElementById('main').scrollIntoView();
</script>
</html>